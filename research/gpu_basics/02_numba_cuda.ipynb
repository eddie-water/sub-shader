{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPU Acceleration Basics 02\n",
    "This notebook follows the video Python CUDA Installation & CUPY | GPU Acceleration Basics 02 by Rounak Paul found on YouTube.\n",
    "\n",
    "## Using CUDA via the Numba Library\n",
    "Going to compare the performace of running a simple operation on a large array (65k values)\n",
    "\n",
    "Host = CPU\n",
    "Device = GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import cuda\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_host = np.ones(shape=(65536))\n",
    "\n",
    "def host_increment_by_one(arr):\n",
    "    for i in range(len(arr)):\n",
    "        arr[i] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.07 ms ± 74.1 μs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "host_increment_by_one(x_host)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU Acceleration (1D Data)\n",
    "Here we are accelerating the previous function by having it run on the GPU referred to as the \"device\". I'm still figuring out how this all works, but I'm beginning to see it. \n",
    "\n",
    "Notice how the setup of this GPU function is different than the previous CPU function. \n",
    "- Before the function, we have to use the Numba CUDA decorator: @cuda.jit. This tells the system to compile the function into GPU machine code using Numba's JIT Compiler. This decorator marks the function as a 'CUDA kernel', which means it can be launched on the GPU from the CPU. \n",
    "- Inside the function, there's all this notion of Thread ID, Block ID, and Block Width. This has to do with all the total number of threads (or cores?) we are sending this computation to. These are needed to properly correspond the indexes of the array to the cores that do the computation.\n",
    "- The main part of the function, it is no longer a sequential for loop because this \"code\" is not really run \"sequentially\", so it isn't really written sequentially. It's being compiled into machine code that executes instructions in parallel as a \"batch\". So calling it \"code\" is somewhat of a misnomer. But it does get compiled, and it does compute results, so it is kinda of like code. As for the device, I'm imagining a big grid of cores, and every single one of them corresponds to one element in the array, specified by our function and the index calculation below. Each core is tasked with performing a computation on a single element in the array. In this case, it is simply incrementing the value of each element by one, simultaneously all at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def device_increment_by_one(arr):\n",
    "    # Thread ID in a 1D Block\n",
    "    tid = cuda.threadIdx.x\n",
    "\n",
    "    # Block ID in a 1D Grid\n",
    "    bid = cuda.blockIdx.x\n",
    "\n",
    "    # Block Width (Number of Threads per Block)\n",
    "    bw = cuda.blockDim.x\n",
    "\n",
    "    # Compute flattened index in side the array\n",
    "    i = tid + bid * bw\n",
    "    if i < arr.size:    # Boundary Condition\n",
    "        arr[i] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_host = np.ones(shape=(65536))\n",
    "x_device = cuda.to_device(x_host)\n",
    "threads_per_block = 256\n",
    "blocks_per_grid = (x_device.size + (threads_per_block - 1)) // threads_per_block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Single Trial Run vs Multi Trial Run\n",
    "The magic command '%%time' runs the block once, where '%%timeit' runs it multiple times, and prints the mean and standard deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 80.6 ms, sys: 28.5 ms, total: 109 ms\n",
      "Wall time: 316 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "device_increment_by_one[blocks_per_grid, threads_per_block](x_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.1 μs ± 1.25 μs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "device_increment_by_one[blocks_per_grid, threads_per_block](x_device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the single trial block took **milliseconds** where the multi trial block took **microseconds**\n",
    "\n",
    "In the single trial block, the function had to be compiled, and then transferred to the device (over the PCIe bus), which is why it took so long the first trial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative GPU Index Technique\n",
    "This is another way you can get the thread index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def device_increment_by_one(arr):\n",
    "    # Quick and dirty way to get the thread indeces\n",
    "    i = cuda.grid(1)\n",
    "\n",
    "    if i < arr.size:\n",
    "        arr[i] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 80.6 ms, sys: 28.5 ms, total: 109 ms\n",
      "Wall time: 316 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "device_increment_by_one[blocks_per_grid, threads_per_block](x_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.3 μs ± 204 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "device_increment_by_one[blocks_per_grid, threads_per_block](x_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2., 2., 2., ..., 2., 2., 2.], shape=(65536,))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Since the previous time/timeits ran a modified the data a bunch of times already, re-init the data\n",
    "x_host = np.ones(shape=(65536))\n",
    "x_device = cuda.to_device(x_host)\n",
    "threads_per_block = 256\n",
    "blocks_per_grid = (x_device.size + (threads_per_block - 1)) // threads_per_block\n",
    "\n",
    "# Run the kernel on the re-initialized data\n",
    "device_increment_by_one[blocks_per_grid, threads_per_block](x_device)\n",
    "x_device.copy_to_host(x_host)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU Acceleration (2D Data)\n",
    "Now do the same except a 2D arrray, most of it similar, besides the thread per block stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a 2D array\n",
    "# Note: The shape of the array is (256, 256) but the size is 65536\n",
    "x_device = cuda.to_device(np.ones(shape=(256, 256)))\n",
    "\n",
    "@cuda.jit\n",
    "def device_increment_2D_arr(arr):\n",
    "    x, y = cuda.grid(2)\n",
    "    if x < arr.shape[0] and y < arr.shape[1]:\n",
    "        arr[x, y] += 1\n",
    "\n",
    "threads_per_block = (16, 16)\n",
    "blocks_per_grid = (math.ceil(x_device.shape[0] / threads_per_block[0]),\n",
    "                   math.ceil(x_device.shape[1] / threads_per_block[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.9 μs ± 534 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "device_increment_2D_arr[blocks_per_grid, threads_per_block](x_device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CuPy and Numba are friends\n",
    "CuPy arrays use CUDA Array Interfaces, so they can be easily converted into Numba arrays. They are both built on top the CUDA toolkit which is provided by Nvidia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'cupy.ndarray'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[2., 2., 2., ..., 2., 2., 2.],\n",
       "       [2., 2., 2., ..., 2., 2., 2.],\n",
       "       [2., 2., 2., ..., 2., 2., 2.],\n",
       "       ...,\n",
       "       [2., 2., 2., ..., 2., 2., 2.],\n",
       "       [2., 2., 2., ..., 2., 2., 2.],\n",
       "       [2., 2., 2., ..., 2., 2., 2.]], shape=(256, 256))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cupy as cp\n",
    "\n",
    "x_device = cp.ones(shape=(256, 256))\n",
    "device_increment_2D_arr[blocks_per_grid, threads_per_block](x_device)\n",
    "\n",
    "print(type(x_device))\n",
    "x_device"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
